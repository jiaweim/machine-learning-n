# ä½¿ç”¨ RNN ç”Ÿæˆæ–‡æœ¬

- [ä½¿ç”¨ RNN ç”Ÿæˆæ–‡æœ¬](#ä½¿ç”¨-rnn-ç”Ÿæˆæ–‡æœ¬)
  - [1. ç®€ä»‹](#1-ç®€ä»‹)
  - [2. åˆå§‹è®¾ç½®](#2-åˆå§‹è®¾ç½®)
    - [2.1 å¯¼å…¥åŒ…](#21-å¯¼å…¥åŒ…)
    - [2.2 ä¸‹è½½æ•°æ®é›†](#22-ä¸‹è½½æ•°æ®é›†)
    - [2.3 è¯»å–æ•°æ®](#23-è¯»å–æ•°æ®)
  - [3. å¤„ç†æ–‡æœ¬](#3-å¤„ç†æ–‡æœ¬)
    - [3.1 å‘é‡åŒ–æ–‡æœ¬](#31-å‘é‡åŒ–æ–‡æœ¬)
    - [3.2 é¢„æµ‹ä»»åŠ¡](#32-é¢„æµ‹ä»»åŠ¡)
    - [3.3 åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼](#33-åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼)
    - [3.4 åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡](#34-åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡)
  - [4. æ„å»ºæ¨¡å‹](#4-æ„å»ºæ¨¡å‹)
  - [5. è¯•ç”¨æ¨¡å‹](#5-è¯•ç”¨æ¨¡å‹)
  - [6. è®­ç»ƒæ¨¡å‹](#6-è®­ç»ƒæ¨¡å‹)
    - [6.1 è®¾ç½® optimizer å’Œ loss function](#61-è®¾ç½®-optimizer-å’Œ-loss-function)
    - [6.2 è®¾ç½® checkpoints](#62-è®¾ç½®-checkpoints)
    - [6.3 å¼€å§‹è®­ç»ƒ](#63-å¼€å§‹è®­ç»ƒ)
  - [7. ç”Ÿæˆæ–‡æœ¬](#7-ç”Ÿæˆæ–‡æœ¬)
  - [8. å¯¼å‡ºç”Ÿæˆå™¨](#8-å¯¼å‡ºç”Ÿæˆå™¨)
  - [9. é«˜çº§ï¼šè‡ªå®šä¹‰è®­ç»ƒ](#9-é«˜çº§è‡ªå®šä¹‰è®­ç»ƒ)
  - [10. å‚è€ƒ](#10-å‚è€ƒ)

Last updated: 2022-07-20, 17:21
@author Jiawei Mao
****

## 1. ç®€ä»‹

ä¸‹é¢æ¼”ç¤ºä½¿ç”¨åŸºäºå­—ç¬¦çš„ RNN ç”Ÿæˆæ–‡æœ¬ã€‚ä½¿ç”¨ Andrej Karpathy çš„åšå®¢ [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) ä¸­ä½¿ç”¨çš„ Shakespeare çš„ä¸€ç¯‡æ–‡ç« ä½œä¸ºæ•°æ®é›†ã€‚ç»™å®šè¯¥æ•°æ®ï¼ˆ"Shakespear"ï¼‰çš„ä¸€ä¸ªå­—ç¬¦åºåˆ—ï¼Œè®­ç»ƒæ¨¡å‹é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆ"e"ï¼‰ã€‚åå¤è°ƒç”¨æ¨¡å‹å°±èƒ½ç”Ÿæˆè¾ƒé•¿çš„æ–‡æœ¬åºåˆ—ã€‚

ä¸‹é¢ä½¿ç”¨ `tf.keras` å®ç°æ¨¡å‹ï¼Œä»¥ä¸‹æ–‡æœ¬æ˜¯æ¨¡å‹è®­ç»ƒ 30 ä¸ª epochs åä½¿ç”¨æç¤º "Q" å¼€å§‹è·å¾—çš„è¾“å‡ºï¼š

```txt
QUEENE:
I had thought thou hadst a Roman; for the oracle,
Thus by All bids the man against the word,
Which are so weak of care, by old care done;
Your children were in your holy love,
And the precipitation through the bleeding throne.

BISHOP OF ELY:
Marry, and will, my lord, to weep in such a one were prettiest;
Yet now I was adopted heir
Of the world's lamentable day,
To watch the next way with his father with his face?

ESCALUS:
The cause why then we are all resolved more sons.

VOLUMNIA:
O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,
And love and pale as any will to that word.

QUEEN ELIZABETH:
But how long have I heard the soul for this world,
And show his hands of life be proved to stand.

PETRUCHIO:
I say he look'd on, if I must be content
To stay him from the fatal of our country's bliss.
His lordship pluck'd from this sentence then for prey,
And then let us twain, being the moon,
were she such a case as fills m
```

ä¸Šé¢ç”Ÿæˆçš„æ–‡æœ¬ï¼Œè™½ç„¶æœ‰äº›å¥å­è¯­æ³•æ­£ç¡®ï¼Œä½†æ˜¯å¤§å¤šæ•°æ²¡æœ‰æ„ä¹‰ï¼Œå³æ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°å•è¯çš„å«ä¹‰ï¼Œä½†æ˜¯è€ƒè™‘åˆ°ï¼š

- æ¨¡å‹æ˜¯åŸºäºå­—ç¬¦çš„ã€‚æ¨¡å‹å¹¶ä¸çŸ¥é“å¦‚ä½•æ‹¼å†™è‹±æ–‡å•è¯ï¼Œç”šè‡³ä¸çŸ¥é“è¿™äº›å•è¯æ˜¯æ–‡æœ¬çš„åŸºæœ¬ç»„æˆï¼›
- è®­ç»ƒæ•°æ®é›†æ‰¹é‡è¾ƒå°ï¼ˆæ¯ä¸ª 100 å­—ç¬¦ï¼‰ã€‚

## 2. åˆå§‹è®¾ç½®

### 2.1 å¯¼å…¥åŒ…

```python
import tensorflow as tf

import numpy as np
import os
import time
```

### 2.2 ä¸‹è½½æ•°æ®é›†

```python
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 
    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
```

```txt
Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt
1122304/1115394 [==============================] - 0s 0us/step
1130496/1115394 [==============================] - 0s 0us/step
```

### 2.3 è¯»å–æ•°æ®

é¦–å…ˆæŸ¥çœ‹æ–‡æœ¬ï¼š

```python
# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# å­—ç¬¦æ•°
print(f'Length of text: {len(text)} characters')
```

```txt
Length of text: 1115394 characters
```

æŸ¥çœ‹æ–‡æœ¬çš„å‰ 250 ä¸ªå­—ç¬¦ï¼š

```python
print(text[:250])
```

```txt
First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.
```

æ–‡ä»¶ä¸­å­—ç¬¦ç§ç±»æ•°ï¼š

```python
vocab = sorted(set(text))
print(f'{len(vocab)} unique characters')
```

```txt
65 unique characters
```

## 3. å¤„ç†æ–‡æœ¬

### 3.1 å‘é‡åŒ–æ–‡æœ¬

åœ¨è®­ç»ƒå‰éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—è¡¨ç¤ºã€‚ä½¿ç”¨ `tf.keras.layers.StringLookup` å°†å­—ç¬¦è½¬æ¢ä¸ºæ•°å­— IDã€‚

å…ˆå°†æ–‡æœ¬æ‹†åˆ†ä¸º tokensï¼š

```python
example_texts = ['abcdefg', 'xyz']

chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')
chars
```

```txt
<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>
```

ç„¶ååˆ›å»º `tf.keras.layers.StringLookup`ï¼š

```python
ids_from_chars = tf.keras.layers.StringLookup(
    vocabulary=list(vocab), mask_token=None)
```

è¯¥ layer è´Ÿè´£å°† tokens è½¬æ¢ä¸ºæ•°å­— IDï¼š

```python
ids = ids_from_chars(chars)
ids
```

```txt
<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>
```

ç”±äºæœ¬æ•™ç¨‹æ„å»ºæ¨¡å‹çš„ç›®çš„æ˜¯ç”Ÿæˆæ–‡æœ¬ï¼Œå› æ­¤è¿˜éœ€è¦å°†æ•°å­— ID è½¬æ¢ä¸ºå­—ç¬¦çš„é€†æ“ä½œã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨ `tf.keras.layers.StringLookup(..., invert=True)`ã€‚

ä¸ºäº†ä¿è¯ä¸¤ä¸ª `StringLookup` å…·æœ‰ç›¸åŒçš„è¯æ±‡è¡¨ï¼Œä¸‹é¢ä½¿ç”¨ `get_vocabulary()` è·å¾—ä¸Šé¢çš„è¯æ±‡è¡¨ï¼š

```python
chars_from_ids = tf.keras.layers.StringLookup(
    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)
```

è¯¥ layer å°†æ•°å­— ID è½¬æ¢ä¸ºå­—ç¬¦ï¼Œè¿”å›å­—ç¬¦ç±»å‹çš„ `tf.RaggedTensor`ï¼š

```python
chars = chars_from_ids(ids)
chars
```

```txt
<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>
```

ç”¨ `tf.strings.reduce_join` å°†å­—ç¬¦è¿æ¥ä¸ºå­—ç¬¦ä¸²ï¼š

```python
tf.strings.reduce_join(chars, axis=-1).numpy()
```

```txt
array([b'abcdefg', b'xyz'], dtype=object)
```

```python
def text_from_ids(ids):
  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)
```

### 3.2 é¢„æµ‹ä»»åŠ¡

ç»™å®šä¸€ä¸ªå­—ç¬¦æˆ–ä¸€ä¸²å­—ç¬¦ï¼Œä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„å­—ç¬¦æ˜¯ä»€ä¹ˆï¼Ÿè¿™å°±æ˜¯æ¨¡å‹æ‰€éœ€æ‰§è¡Œçš„ä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦åºåˆ—ï¼Œéœ€è¦è®­ç»ƒè¯¥æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„å­—ç¬¦æ˜¯ä»€ä¹ˆã€‚

ç”±äº RNN ç»´æŠ¤äº†ä¸€ä¸ªå†…éƒ¨çŠ¶æ€ï¼Œè¯¥çŠ¶æ€ä¾èµ–äºå…ˆå‰çœ‹åˆ°çš„å…ƒç´ ï¼Œç»™å®šå½“å‰çœ‹åˆ°çš„æ‰€æœ‰å­—ç¬¦ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

### 3.3 åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼

ä¸‹é¢å°†æ–‡æœ¬æ‹†åˆ†ä¸ºæ ·æœ¬åºåˆ—ã€‚æ¯ä¸ªè¾“å…¥åºåˆ—åŒ…å«æ¥è‡ªæ–‡æœ¬çš„ `seq_length` ä¸ªå­—ç¬¦ã€‚

å¯¹æ¯ä¸ªè¾“å…¥åºåˆ—ï¼Œå¯¹åº”çš„ç›®æ ‡åŒ…å«ç›¸åŒé•¿åº¦çš„æ–‡æœ¬ï¼Œåªæ˜¯å‘å³ç§»äº†ä¸€ä¸ªå­—ç¬¦ã€‚å‡è®¾ `seq_length` ä¸º 4ï¼Œæ–‡æœ¬ä¸º "Hello"ã€‚åˆ™è¾“å…¥ä¸º "Hell"ï¼Œç›®æ ‡åºåˆ—ä¸º "ello"ã€‚

ä¸ºæ­¤ï¼Œé¦–å…ˆä½¿ç”¨ `tf.data.Dataset.from_tensor_slices` å‡½æ•°å°†æ–‡æœ¬å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•æµã€‚

```python
all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))
all_ids
```

```txt
<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>
```

```python
ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)
```

```python
for ids in ids_dataset.take(10):
    print(chars_from_ids(ids).numpy().decode('utf-8'))
```

```txt
F
i
r
s
t
 
C
i
t
i
```

```python
seq_length = 100
```

ä½¿ç”¨ `batch` æ–¹æ³•å¯ä»¥è½»æ¾å°†è¿™äº›å•ä¸ªå­—ç¬¦è½¬æ¢ä¸ºæŒ‡å®šé•¿åº¦çš„åºåˆ—ï¼š

```python
sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)

for seq in sequences.take(1):
    print(chars_from_ids(seq))
```

```txt
tf.Tensor(
[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'
 b'\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'
 b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'
 b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'
 b'e' b'a' b'k' b'.' b'\n' b'\n' b'A' b'l' b'l' b':' b'\n' b'S' b'p' b'e'
 b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\n' b'\n' b'F' b'i'
 b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\n' b'Y'
 b'o' b'u' b' '], shape=(101,), dtype=string)
```

å°†ä¸Šé¢çš„ tokens è¿æ¥æˆå­—ç¬¦ä¸²ï¼Œæ›´å®¹æ˜“çœ‹å‡ºæ•ˆæœï¼š

```python
for seq in sequences.take(5):
    print(text_from_ids(seq).numpy())
```

```txt
b'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
b'are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k'
b"now Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us ki"
b"ll him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be d"
b'one: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citi'
```

ä¸ºäº†è®­ç»ƒï¼Œéœ€è¦ç”Ÿæˆ `(input, label)` å½¢å¼çš„æˆå¯¹æ•°æ®ï¼Œ`input` å’Œ `label` éƒ½æ˜¯åºåˆ—ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè¾“å…¥æ˜¯å½“å‰å­—ç¬¦ï¼Œè¾“å‡º label æ˜¯ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

ä¸‹é¢çš„å‡½æ•°ï¼Œå°†è¾“å…¥åºåˆ—å¤åˆ¶å¹¶ç§»åŠ¨ 1 ä½ï¼Œä»è€Œå°†æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥å’Œ label å¯¹é½ï¼š

```python
def split_input_target(sequence):
    input_text = sequence[:-1]
    target_text = sequence[1:]
    return input_text, target_text
```

```python
split_input_target(list("Tensorflow"))
```

```txt
(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],
 ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])
```

```python
dataset = sequences.map(split_input_target)
```

```python
for input_example, target_example in dataset.take(1):
    print("Input :", text_from_ids(input_example).numpy())
    print("Target:", text_from_ids(target_example).numpy())
```

```txt
Input : b'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou'
Target: b'irst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
```

### 3.4 åˆ›å»ºè®­ç»ƒæ‰¹æ¬¡

ä¸Šé¢å·²ä½¿ç”¨ `tf.data` å°†æ–‡æœ¬æ‹†åˆ†ä¸ºåºåˆ—é›†åˆã€‚å°†æ•°æ®è¾“å…¥æ¨¡å‹ä¹‹å‰ï¼Œè¿˜éœ€è¦å°†æ•°æ®æ‰“ä¹±ï¼Œå¹¶æ‰“åŒ…æˆ batchesã€‚

```python
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data è®¾è®¡ä¸ºå¤„ç†æ— é™åºåˆ—çš„å¯èƒ½
# å› æ­¤ä¸ä¼šåœ¨å†…å­˜ä¸­æ‰“ä¹±æ•´ä¸ªåºåˆ—ï¼Œè€Œæ˜¯ç»´æŠ¤ä¸€ä¸ªç¼“å†²åŒºï¼Œåœ¨å…¶ä¸­å¯¹å…ƒç´ è¿›è¡Œæ´—ç‰Œ
BUFFER_SIZE = 10000

dataset = (
    dataset
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.experimental.AUTOTUNE))

dataset
```

```txt
<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>
```

## 4. æ„å»ºæ¨¡å‹

ä¸‹é¢é€šè¿‡æ‰©å±• `keras.Model` ç±»å®šä¹‰æ¨¡å‹ã€‚

è¯¥æ¨¡å‹åŒ…å«ä¸‰å±‚ï¼š

- `tf.keras.layers.Embedding`ï¼šè¾“å…¥å±‚ã€‚å¯è®­ç»ƒçš„æŸ¥æ‰¾è¡¨ï¼Œç”¨äºå°†å­—ç¬¦ ID æ˜ å°„åˆ°ç»´åº¦ä¸º `embedding_dim` çš„å‘é‡ï¼›
- `tf.keras.layers.GRU`ï¼šä¸€ç§ RNNï¼Œå¤§å°ä¸º `units=rnn_units`ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ LSTMã€‚
- `tf.keras.layers.Dense`ï¼šè¾“å‡ºå±‚ï¼Œå¤§å°ä¸º `vocab_size`ã€‚å®ƒä¸ºè¯æ±‡è¡¨çš„æ¯ä¸ªå­—ç¬¦è¾“å‡ºä¸€ä¸ª logitã€‚

```python
# ä»¥å­—ç¬¦è¡¨ç¤ºçš„è¯æ±‡è¡¨é•¿åº¦
vocab_size = len(vocab)

# åµŒå…¥ç»´åº¦
embedding_dim = 256

# RNN å•å…ƒæ•°
rnn_units = 1024
```

```python
class MyModel(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, rnn_units):
        super().__init__(self)
        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
        self.gru = tf.keras.layers.GRU(rnn_units,
                                       return_sequences=True,
                                       return_state=True)
        self.dense = tf.keras.layers.Dense(vocab_size)

    def call(self, inputs, states=None, return_state=False, training=False):
        x = inputs
        x = self.embedding(x, training=training)
        if states is None:
            states = self.gru.get_initial_state(x)
        x, states = self.gru(x, initial_state=states, training=training)
        x = self.dense(x, training=training)

        if return_state:
            return x, states
        else:
            return x
```

```python
model = MyModel(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    rnn_units=rnn_units)
```

å¯¹æ¯ä¸ªå­—ç¬¦ï¼Œæ¨¡å‹æŸ¥æ‰¾åµŒå…¥ï¼Œå°†åµŒå…¥è¾“å…¥ GRU è¿è¡Œä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå†è¾“å…¥ Dense å±‚ç”Ÿæˆ logits æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼š

![](images/2022-03-09-23-21-45.png)

> ğŸ˜Š:è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ `keras.Sequential` æ¨¡å‹ã€‚ä¸ºäº†ç¨åèƒ½ç”Ÿæˆæ–‡æœ¬ï¼Œéœ€è¦ç®¡ç† RNN å†…éƒ¨çŠ¶æ€ã€‚æå‰åŒ…å«çŠ¶æ€çš„è¾“å…¥å’Œè¾“å‡ºé€‰é¡¹æ¯”ç¨åé‡æ•´æ¨¡å‹è¦ç®€å•å¾—å¤šã€‚

## 5. è¯•ç”¨æ¨¡å‹

è¿è¡Œæ¨¡å‹ï¼Œçœ‹çœ‹å®ƒçš„è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

é¦–å…ˆæ£€æŸ¥è¾“å‡º shapeï¼š

```python
for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")
```

```txt
(64, 100, 66) # (batch_size, sequence_length, vocab_size)
```

åœ¨ä¸Šä¾‹ä¸­ï¼Œè¾“å‡ºåºåˆ—é•¿åº¦ä¸º 100ï¼Œä½†æ¨¡å‹å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥ï¼š

```python
model.summary()
```

```txt
Model: "my_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       multiple                  16896     
                                                                 
 gru (GRU)                   multiple                  3938304   
                                                                 
 dense (Dense)               multiple                  67650     
                                                                 
=================================================================
Total params: 4,022,850
Trainable params: 4,022,850
Non-trainable params: 0
_________________________________________________________________
```

ä¸ºäº†ä»æ¨¡å‹è·å¾—å®é™…çš„é¢„æµ‹å€¼ï¼Œéœ€è¦ä»è¾“å‡ºåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä»¥è·å¾—å®é™…å­—ç¬¦ç´¢å¼•ã€‚è¯¥åˆ†å¸ƒç”±åœ¨è¯æ±‡è¡¨ä¸Šçš„ logit å®šä¹‰ã€‚

> ä»è¾“å‡ºåˆ†å¸ƒä¸­å–æ ·å¾ˆé‡è¦ï¼Œå› ä¸ºç›´æ¥ç”¨ argmax å–å€¼å¾ˆå®¹æ˜“ä½¿æ¨¡å‹é™·å…¥å¾ªç¯ã€‚

è¯•ä¸€ä¸‹è¿™æ‰¹æ•°æ®çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼š

```python
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
```

åœ¨æ¯ä¸ªæ—¶é—´æ­¥æä¾›äº†ä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹ç´¢å¼•å€¼ï¼š

```python
sampled_indices
```

```txt
array([29, 23, 11, 14, 42, 27, 56, 29, 14,  6,  9, 65, 22, 15, 34, 64, 44,
       41, 11, 51, 10, 44, 42, 56, 13, 50,  1, 33, 45, 23, 28, 43, 12, 62,
       45, 60, 43, 62, 38, 19, 50, 35, 19, 14, 60, 56, 10, 64, 39, 56,  2,
       51, 63, 42, 39, 64, 43, 20, 20, 17, 40, 15, 52, 46,  7, 25, 34, 43,
       11, 11, 31, 34, 38, 44, 22, 49, 23,  4, 27,  0, 31, 39,  5,  9, 43,
       58, 33, 30, 49,  6, 63,  5, 50,  4,  6, 14, 62,  3,  7, 35])
```

è§£ç ï¼ŒæŸ¥çœ‹è¿™ä¸ªæœªç»è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹çš„æ–‡æœ¬ï¼š

```python
print("Input:\n", text_from_ids(input_example_batch[0]).numpy())
print()
print("Next Char Predictions:\n", text_from_ids(sampled_indices).numpy())
```

```txt
Input:
 b" us,\nAnd tell 's a tale.\n\nMAMILLIUS:\nMerry or sad shall't be?\n\nHERMIONE:\nAs merry as you will.\n\nMAMI"

Next Char Predictions:
 b"qqsMO[UNK]DDuL!zsSVw:'Gl3KSybJ,;?Z[UNK]TbGF\nMV NvYpUDNLO,lOmm3Sqrpqm?CpDEn[UNK]au[UNK]k,uP?3Fv\nHYfvssTkU'qJqxv[UNK]f'Uvj"
```

## 6. è®­ç»ƒæ¨¡å‹

æ­¤æ—¶ï¼Œè¦è§£å†³çš„é—®é¢˜å¯ä»¥è§†ä¸ºä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»é—®é¢˜ã€‚ç»™å®šå‰é¢çš„ RNN çŠ¶æ€å’Œå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç±»åˆ«ã€‚

### 6.1 è®¾ç½® optimizer å’Œ loss function

æ ‡å‡† `tf.keras.losses.sparse_categorical_crossentropy` æŸå¤±å‡½æ•°é€‚åˆè¯¥æƒ…å†µï¼Œå®ƒåº”ç”¨äºé¢„æµ‹çš„æœ€åä¸€ä¸ªç»´åº¦ã€‚

ç”±äºæ¨¡å‹è¿”å› logitï¼Œæ‰€ä»¥è¦æ·»åŠ  `from_logits` æ ‡ç­¾ï¼š

```python
loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
```

```python
example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("Mean loss:        ", example_batch_mean_loss)
```

```txt
Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)
Mean loss:         tf.Tensor(4.1895466, shape=(), dtype=float32)
```

åˆšåˆå§‹åŒ–çš„æ¨¡å‹è¾“å‡º logit æ¥è¿‘éšæœºåˆ†å¸ƒã€‚ä¸ºäº†è¯å®è¿™ä¸€ç‚¹ï¼Œå¯ä»¥æ£€æŸ¥å¹³å‡æŸå¤±çš„æŒ‡æ•°åº”è¯¥æ¥è¿‘è¯æ±‡è¡¨çš„å¤§å°ã€‚æ›´é«˜çš„æŸå¤±å‡å€¼æ„å‘³ç€æ¨¡å‹ç¡®å®šå…¶ç­”æ¡ˆæ˜¯é”™è¯¯çš„ï¼Œè¯´æ˜æ²¡åˆå§‹åŒ–å¥½ï¼š

```python
tf.exp(example_batch_mean_loss).numpy()
```

```txt
65.99286
```

ä½¿ç”¨ `tf.keras.Model.compile` é…ç½®è®­ç»ƒå‚æ•°ã€‚ä½¿ç”¨ `tf.keras.optimizers.Adam` å’Œä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼š

```python
model.compile(optimizer='adam', loss=loss)
```

### 6.2 è®¾ç½® checkpoints

ä½¿ç”¨ `tf.keras.callbacks.ModelCheckpoint` åœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ£€æŸ¥ç‚¹ï¼š

```python
# checkpoints ä¿å­˜ç›®å½•
checkpoint_dir = './training_checkpoints'
# checkpoint æ–‡ä»¶å
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)
```

### 6.3 å¼€å§‹è®­ç»ƒ

ä¸ºäº†èƒ½åœ¨è¾ƒçŸ­çš„æ—¶é—´è®­ç»ƒå®Œï¼Œè¿™é‡Œåªä½¿ç”¨ 10 ä¸ª epochsã€‚

```python
EPOCHS = 20
```

```python
history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
```

```txt
Epoch 1/20
172/172 [==============================] - 11s 49ms/step - loss: 2.7276
Epoch 2/20
172/172 [==============================] - 9s 49ms/step - loss: 1.9853
Epoch 3/20
172/172 [==============================] - 9s 49ms/step - loss: 1.7084
Epoch 4/20
172/172 [==============================] - 9s 49ms/step - loss: 1.5496
Epoch 5/20
172/172 [==============================] - 9s 49ms/step - loss: 1.4513
Epoch 6/20
172/172 [==============================] - 9s 50ms/step - loss: 1.3830
Epoch 7/20
172/172 [==============================] - 9s 50ms/step - loss: 1.3311
Epoch 8/20
172/172 [==============================] - 9s 50ms/step - loss: 1.2864
Epoch 9/20
172/172 [==============================] - 9s 50ms/step - loss: 1.2466
Epoch 10/20
172/172 [==============================] - 9s 50ms/step - loss: 1.2074
Epoch 11/20
172/172 [==============================] - 9s 50ms/step - loss: 1.1679
Epoch 12/20
172/172 [==============================] - 9s 50ms/step - loss: 1.1264
Epoch 13/20
172/172 [==============================] - 9s 51ms/step - loss: 1.0839
Epoch 14/20
172/172 [==============================] - 9s 51ms/step - loss: 1.0383
Epoch 15/20
172/172 [==============================] - 9s 51ms/step - loss: 0.9894
Epoch 16/20
172/172 [==============================] - 9s 50ms/step - loss: 0.9389
Epoch 17/20
172/172 [==============================] - 9s 51ms/step - loss: 0.8877
Epoch 18/20
172/172 [==============================] - 9s 51ms/step - loss: 0.8347
Epoch 19/20
172/172 [==============================] - 10s 51ms/step - loss: 0.7840
Epoch 20/20
172/172 [==============================] - 9s 51ms/step - loss: 0.7356
```

## 7. ç”Ÿæˆæ–‡æœ¬

ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨å¾ªç¯ä¸­è¿è¡Œï¼Œå¹¶åœ¨æ‰§è¡Œæ—¶è·Ÿè¸ªæ¨¡å‹å†…éƒ¨çŠ¶æ€ã€‚

![](images/2022-03-10-23-32-56.png)

æ¯æ¬¡è°ƒç”¨æ¨¡å‹ï¼Œä¼ å…¥ä¸€ä¸ªæ–‡æœ¬å’Œå†…éƒ¨çŠ¶æ€ï¼Œæ¨¡å‹è¿”å›ä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹åŠæ–°çŠ¶æ€ï¼Œå°†é¢„æµ‹å’ŒçŠ¶æ€ä¼ å›æ¨¡å‹ç»§ç»­ç”Ÿæˆæ–‡æœ¬ã€‚

ä»¥ä¸‹æ˜¯å•æ­¥é¢„æµ‹ï¼š

```python
class OneStep(tf.keras.Model):
    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):
        super().__init__()
        self.temperature = temperature
        self.model = model
        self.chars_from_ids = chars_from_ids
        self.ids_from_chars = ids_from_chars

        # Create a mask to prevent "[UNK]" from being generated.
        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]
        sparse_mask = tf.SparseTensor(
            # Put a -inf at each bad index.
            values=[-float('inf')] * len(skip_ids),
            indices=skip_ids,
            # Match the shape to the vocabulary
            dense_shape=[len(ids_from_chars.get_vocabulary())])
        self.prediction_mask = tf.sparse.to_dense(sparse_mask)

    @tf.function
    def generate_one_step(self, inputs, states=None):
        # Convert strings to token IDs.
        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')
        input_ids = self.ids_from_chars(input_chars).to_tensor()

        # Run the model.
        # predicted_logits.shape is [batch, char, next_char_logits]
        predicted_logits, states = self.model(inputs=input_ids, states=states,
                                              return_state=True)
        # Only use the last prediction.
        predicted_logits = predicted_logits[:, -1, :]
        predicted_logits = predicted_logits / self.temperature
        # Apply the prediction mask: prevent "[UNK]" from being generated.
        predicted_logits = predicted_logits + self.prediction_mask

        # Sample the output logits to generate token IDs.
        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)
        predicted_ids = tf.squeeze(predicted_ids, axis=-1)

        # Convert from token ids to characters
        predicted_chars = self.chars_from_ids(predicted_ids)

        # Return the characters and model state.
        return predicted_chars, states
```

```python
one_step_model = OneStep(model, chars_from_ids, ids_from_chars)
```

åœ¨å¾ªç¯ä¸­è¿è¡Œä»¥ç”Ÿæˆæ–‡æœ¬ï¼ŒæŸ¥çœ‹ç”Ÿæˆçš„æ–‡æœ¬ï¼Œå¯ä»¥çœ‹åˆ°æ¨¡å‹ç›´åˆ°ä½•æ—¶å¤§å†™ã€ç”Ÿæˆæ®µè½ä»¥åŠæ¨¡ä»¿èå£«æ¯”äºšå¼çš„å†™ä½œè¯æ±‡ã€‚ç”±äºè®­ç»ƒçš„ epochs è¾ƒå°‘ï¼Œå®ƒè¿˜æ²¡å­¦ä¼šå½¢æˆè¿è´¯çš„å¥å­ï¼š

```python
start = time.time()
states = None
next_char = tf.constant(['ROMEO:'])
result = [next_char]

for n in range(1000):
    next_char, states = one_step_model.generate_one_step(next_char, states=states)
    result.append(next_char)

result = tf.strings.join(result)
end = time.time()
print(result[0].numpy().decode('utf-8'), '\n\n' + '_' * 80)
print('\nRun time:', end - start)
```

```txt
ROMEO:
Let's to Clarence! a cotcher,
Ne'er through your youth'd sweet use yept.

GREGORY:
I hold thee coward: Grumio, perisome,
This small sheque have water'd indeed is danger, and I
do erjat for thee to behold this sport
hainston in himself, as thou art day too doers it,
Should yet with such sweet give I stray; let's stay to lay
The angly cowds for maids, some liberty.

TYBALT:
Peace, sir! 'I, sir.

GRUMIO:
Why, she's done with him!

First Lord:
Peace! come, weakness comes to your walls,
Ruch news be possessing now with two hours;
Whiles, noble lord of fortune enemies
Will some old coke in spite with me.
An officer of it, an oyself not tears:
O, what myseet pleasure we hence as if
He whom by deed is dead; and then, O grieves me
swear for grieving
And laments that Marcius shall
Your knees are coming into A fun
tremble and talk of faults upon his stoney?
Ancogio, wine, within their helses of thy gross
Are more behilled him to come;
For who is careful note?

LUCIO:
Let's be gone. King Richard  

________________________________________________________________________________

Run time: 3.747878074645996
```

æ”¹å–„è¾“å‡ºçš„æœ€ç®€å•æ–¹æ³•æ˜¯å»¶é•¿è®­ç»ƒæ—¶é—´ï¼Œå¦‚ `EPOCHS=30`ã€‚

å¯ä»¥å°è¯•ä½¿ç”¨å…¶å®ƒçš„å­—ç¬¦ä¸²å¼€å§‹å¯åŠ¨ï¼Œå°è¯•å†æ·»åŠ ä¸€å±‚ RNN ä»¥æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œæˆ–è€…è°ƒæ•´ temperature å‚æ•°ä»¥ç”Ÿæˆæ›´å¤šæˆ–æ›´å°‘çš„éšæœºé¢„æµ‹ã€‚

å¦‚æœå¸Œæœ›æ¨¡å‹æ›´å¿«åœ°ç”Ÿæˆæ–‡æœ¬ï¼Œæœ€ç®€å•çš„æ–¹æ³•æ˜¯æ‰¹é‡ç”Ÿæˆæ–‡æœ¬ã€‚ä¸‹é¢ä½¿ç”¨ç›¸åŒçš„å‚æ•°ï¼Œä½†ä¸€æ¬¡ç”Ÿæˆ 5 ä¸ªè¾“å‡ºï¼š

```python
start = time.time()
states = None
next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])
result = [next_char]

for n in range(1000):
    next_char, states = one_step_model.generate_one_step(next_char, states=states)
    result.append(next_char)

result = tf.strings.join(result)
end = time.time()
print(result, '\n\n' + '_' * 80)
print('\nRun time:', end - start)
```

```txt
tf.Tensor(
[b"ROMEO:\nNoble Scopeon.\n\nDUKE VINCENTIO:\nLong levy me, let them go.\n\nFirst Servingman:\nLet it be full as little load,\nAnd neither violence carried him ripest,\nThat woe down their swelling griefs:\nWho is't that strewbsh! very welcome to yee,\nMy death is mute on seek.\n\nAUFIDIUS:\nGo in the command.\n\nLADY ANNE:\nNo, not a young modning: girls our company\nTo move my scope with no less acreher with'd;\nWith maids shore branght there delights,\nOr, if thou canst up dwicks.\n\nFirst Son:\nIf I may believe you are gone to Friar Paul,\nTo come to 't. But you are well for Kate no quarrel.\n\nGLOUCESTER:\nI pity, I will make him where alack. I had.\n\nCOMINIUS:\nThese babes article that are equalty comfort?\nWith you a hugh a caitiff wretched purpose,\nwhoring colour with the hour It and\nYour bound upon his. Are you all your son,\nTo see it in your promise. What is't,\nOr lest not that Enward his deliver's blood,\nTo see your minds shall free there.\n\nBUCKINGHAM:\nThou hast that playen you with said Some-back graze;\nA cartag"
 b"ROMEO:\nYea.\n\nLord:\nHence, o' the maiding: our voices I have disclamed.\nNow, my Lord Hadwing Romeo not any thing?\nTherefore depart,\nWho is as charital voice, encommanders,\nAnd make my prison recompense by the world\nYou make the case more voices. This life England,\nToo due eyes to his bastarding. Dir himself\nWhich ever soon means, to make the people,\nBut with upon her joins of life\nAs thou do meet the senate have done,\nRather than we shall have heards' loss that news.\n\nPRISCESET:\nFrom your night!\nFor you, my lord, shall I new an evil,\nI am the stary of 'em!'\nAnd, knowing I maintained with him? You\ncan, till help the more change plucks his heed of fated,\nAn old man great man to assemble,\nThat I do die in badding them;\nYet hener my father, might with saintly all.\n\nPETRUCHIO:\nVery well;\nIndeed, my lord, will go wake this intender\nAs please to empty horse; my soul's love confound agat\nEnforce your good presence strike it to\nCoven'd with the stroke of wall, to diving thou\nArt tullers'd, and give yo"
 b"ROMEO:\nO, 'tis an imploshery; but I, were I\nA second palace a city curged\nIn honour foes! Our prince!\n\nMessenger:\nApollo's pared,s--hou canst give her thorn!\nThe odds repeal the one should teld thee here.\n\nOfficer:\n\nHASTINGS:\nBut my father, Madian and lo, nor night.\nTell Veran Padia affold to him.\n\nKING EDWARD IV:\nClarence's son,\nGood Kate; near to this king?\n\nAUTOLYCUS:\nA so on hid enforce of that are go.\n\nHENRY BOLINGBROKE:\nHow long continue cannot abide it\nTo lay the completion of alms, and lawful bragging\nUnder your bearing with yours. Now look'd parting lay\non thee to using mile home; where no waits in France,\nWhereto please this hard haply may not, sweit dight in Rome.\nThou wast'st over-one forth haste my mind it sweat;\nAnd after men, if you were\na broken faith, being so; still well?\n\nDUKE VINCENTIO:\nYou speak how you coward! O doing od them?\n\nRUTLAND:\nO Dakenes, I do lose thy heir;\nThere is a sort corrupted with Lady, late of me,\nAnd in but were impruments like the room:\nYea, like Her"
 b"ROMEO:\nWhat is the matter?\n\nMENENIUS:\nYou charge you, and for them up.\n\nDUKE VINCENTIO:\nNay, then 'pood the world.\n\nAUFIDIUS:\nOnly pluck to your woo-bad--\nThose children I assued the rogue of my curse!\nWhat! what an Edward's,\nHer and York, then to you and his heir\nAs else to Ravels and seen to become, let it good;\nMore periloveth he did see she with him;\nWhose deadly shadow eyes did need to heavy;\nMerry when deputy else this sport to door; as cut\nAs chaster thinks it, Pompey; cambish, to behold him\nHe'll have said you all so soonest.\n\nROMEO:\nI am thou art not only for a necessary\nbends thee where to bump with pumple to our hold\nNay drew; that which you have discredited: me\nMonercusey and his nest down tradish of a\nbutcheriest wave, i' faith from hence;\nCan this bright land-washey's highness to accomplish,\nOur person: let my complot in Warwick?\nHenceforward, hist, and proceedings be afforts;\nYour spite, that, in that same biligence,\nWill win my wager better vessel, when\nYou have pass'd here i"
 b"ROMEO:\nYet give thee more, mistrust, and cannot speak?\n\nMENENIUS:\nPeace, peace!\nHere is a curse that for some hour i' the sanctuful day,\nThat mortal heir with solessing trice:\nThrough in lite already, yet here it present\nThy beauty, then to Burgundy things that see\nOut of his state, what come from thee most ingent,\nCan cleard my father come from her, all this land and the rest,\nFor the angland shapes with sels abody to death,\nEngland in the devils, thought to behold\nhis even.\n\nNurse:\nMendenly, my might bears me to my truth,\nWhich made thy birth a ten, alike,\nPersuit us not this Romeo will do't?\n\nMENENIUS:\nThey are glad was Scanfait befancish;\nThere is a slave; and let him approve her instruct her.\n\nYURL CISTER:\nHe says she's marriage.\n\nANGELO:\nWhen we are graced me in my chopen of\none skill's new the rest o'er by his house.\nHere come your honour, will I make with still stim.\n\nPost:\nHe may at Laurence' dead time, Richard, how they know\nThat's no bitter cloud reverend roof-boon.\n\nLADY GREY:\nWh"], shape=(5,), dtype=string) 

________________________________________________________________________________

Run time: 3.880802869796753
```

## 8. å¯¼å‡ºç”Ÿæˆå™¨

ä¿å­˜ä¸Šé¢çš„å•æ­¥æ¨¡å‹ï¼Œä»è€Œåœ¨å…¶å®ƒåœ°æ–¹ä½¿ç”¨ï¼š

```python
tf.saved_model.save(one_step_model, 'one_step')
one_step_reloaded = tf.saved_model.load('one_step')
```

```txt
WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000018BE322ACD0>, because it is not built.
WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.
INFO:tensorflow:Assets written to: one_step\assets
INFO:tensorflow:Assets written to: one_step\assets
```

```python
states = None
next_char = tf.constant(['ROMEO:'])
result = [next_char]

for n in range(100):
    next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)
    result.append(next_char)

print(tf.strings.join(result)[0].numpy().decode("utf-8"))
```

```txt
ROMEO:
Why, what were you fie?

ROMEO:
Stay, no less
Than what is dasging with the secret negls
give my as
```

## 9. é«˜çº§ï¼šè‡ªå®šä¹‰è®­ç»ƒ

ä¸Šé¢çš„è®­ç»ƒè¿‡ç¨‹å¾ˆç®€å•ï¼Œä½†æ˜¯è‡ªå®šä¹‰æ€§è¾ƒå·®ã€‚å®ƒä½¿ç”¨ teacher-forcing é˜»æ­¢é”™è¯¯é¢„æµ‹åé¦ˆç»™æ¨¡å‹ï¼Œæ‰€ä»¥æ¨¡å‹æ— æ³•å­¦ä¼šä»é”™è¯¯ä¸­æ¢å¤ã€‚

ç°åœ¨å·²ç»çŸ¥é“å¦‚ä½•æ‰‹åŠ¨è¿è¡Œæ¨¡å‹ï¼Œæ¥ä¸‹æ¥å®ç°è®­ç»ƒå¾ªç¯ã€‚

è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯æœ€é‡è¦çš„æ˜¯è®­ç»ƒæ­¥é•¿å‡½æ•°ã€‚

ä½¿ç”¨ `tf.GradientTape` è®°å½•æ¢¯åº¦ã€‚åŸºæœ¬æ­¥éª¤ï¼š

1. åœ¨ `tf.GradientTape` ä¸‹æ‰§è¡Œæ¨¡å‹å¹¶è®¡ç®—æŸå¤±
2. ä½¿ç”¨ä¼˜åŒ–å™¨è®¡ç®—å¹¶æ›´æ–°æ¨¡å‹

```python
class CustomTraining(MyModel):
    @tf.function
    def train_step(self, inputs):
        inputs, labels = inputs
        with tf.GradientTape() as tape:
            predictions = self(inputs, training=True)
            loss = self.loss(labels, predictions)
        grads = tape.gradient(loss, model.trainable_variables)
        self.optimizer.apply_gradients(zip(grads, model.trainable_variables))

        return {'loss': loss}
```

ä¸Šé¢éµå¾ª Keras çš„ `train_step` ç®¡ç†å®ç° `train_step` æ–¹æ³•ã€‚è¿™æ˜¯å¯é€‰çš„ï¼Œä½†æ˜¯å®ç°å®ƒå¯ä»¥ä¿®æ”¹è®­ç»ƒæ­¥éª¤ï¼Œä¾ç„¶ä½¿ç”¨ keras çš„ `Model.compile` å’Œ `Model.fit` æ–¹æ³•ã€‚

```python
model = CustomTraining(
    vocab_size=len(ids_from_chars.get_vocabulary()),
    embedding_dim=embedding_dim,
    rnn_units=rnn_units)
```

```python
model.compile(optimizer=tf.keras.optimizers.Adam(),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))
```

```python
model.fit(dataset, epochs=1)
```

```txt
172/172 [==============================] - 12s 49ms/step - loss: 2.7215
<keras.callbacks.History at 0x18c1e711a30>
```

å¦‚æœéœ€è¦æ›´å¤šçš„æ§åˆ¶ï¼Œè¿˜å¯ä»¥è‡ªå·±ç¼–å†™å®Œæ•´çš„è®­ç»ƒå¾ªç¯ï¼š

```python
EPOCHS = 10

mean = tf.metrics.Mean()

for epoch in range(EPOCHS):
    start = time.time()

    mean.reset_states()
    for (batch_n, (inp, target)) in enumerate(dataset):
        logs = model.train_step([inp, target])
        mean.update_state(logs['loss'])

        if batch_n % 50 == 0:
            template = f"Epoch {epoch + 1} Batch {batch_n} Loss {logs['loss']:.4f}"
            print(template)

    # saving (checkpoint) the model every 5 epochs
    if (epoch + 1) % 5 == 0:
        model.save_weights(checkpoint_prefix.format(epoch=epoch))

    print()
    print(f'Epoch {epoch + 1} Loss: {mean.result().numpy():.4f}')
    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')
    print("_" * 80)

model.save_weights(checkpoint_prefix.format(epoch=epoch))
```

```txt
Epoch 1 Batch 0 Loss 2.1871
Epoch 1 Batch 50 Loss 2.0298
Epoch 1 Batch 100 Loss 1.9539
Epoch 1 Batch 150 Loss 1.8652

Epoch 1 Loss: 1.9856
Time taken for 1 epoch 10.03 sec
________________________________________________________________________________
Epoch 2 Batch 0 Loss 1.8214
Epoch 2 Batch 50 Loss 1.7291
Epoch 2 Batch 100 Loss 1.6715
Epoch 2 Batch 150 Loss 1.6228

Epoch 2 Loss: 1.7065
Time taken for 1 epoch 9.01 sec
________________________________________________________________________________
Epoch 3 Batch 0 Loss 1.5961
Epoch 3 Batch 50 Loss 1.5937
Epoch 3 Batch 100 Loss 1.5324
Epoch 3 Batch 150 Loss 1.5287

Epoch 3 Loss: 1.5474
Time taken for 1 epoch 9.00 sec
________________________________________________________________________________
Epoch 4 Batch 0 Loss 1.4994
Epoch 4 Batch 50 Loss 1.4740
Epoch 4 Batch 100 Loss 1.4043
Epoch 4 Batch 150 Loss 1.3923

Epoch 4 Loss: 1.4493
Time taken for 1 epoch 8.99 sec
________________________________________________________________________________
Epoch 5 Batch 0 Loss 1.3709
Epoch 5 Batch 50 Loss 1.3757
Epoch 5 Batch 100 Loss 1.3649
Epoch 5 Batch 150 Loss 1.3584

Epoch 5 Loss: 1.3814
Time taken for 1 epoch 9.24 sec
________________________________________________________________________________
Epoch 6 Batch 0 Loss 1.3096
Epoch 6 Batch 50 Loss 1.3219
Epoch 6 Batch 100 Loss 1.3488
Epoch 6 Batch 150 Loss 1.3284

Epoch 6 Loss: 1.3301
Time taken for 1 epoch 9.14 sec
________________________________________________________________________________
Epoch 7 Batch 0 Loss 1.2754
Epoch 7 Batch 50 Loss 1.3036
Epoch 7 Batch 100 Loss 1.3005
Epoch 7 Batch 150 Loss 1.2610

Epoch 7 Loss: 1.2859
Time taken for 1 epoch 9.14 sec
________________________________________________________________________________
Epoch 8 Batch 0 Loss 1.2182
Epoch 8 Batch 50 Loss 1.2486
Epoch 8 Batch 100 Loss 1.2142
Epoch 8 Batch 150 Loss 1.2231

Epoch 8 Loss: 1.2451
Time taken for 1 epoch 9.11 sec
________________________________________________________________________________
Epoch 9 Batch 0 Loss 1.2097
Epoch 9 Batch 50 Loss 1.1847
Epoch 9 Batch 100 Loss 1.1935
Epoch 9 Batch 150 Loss 1.1912

Epoch 9 Loss: 1.2056
Time taken for 1 epoch 9.15 sec
________________________________________________________________________________
Epoch 10 Batch 0 Loss 1.1643
Epoch 10 Batch 50 Loss 1.1833
Epoch 10 Batch 100 Loss 1.1651
Epoch 10 Batch 150 Loss 1.1732

Epoch 10 Loss: 1.1661
Time taken for 1 epoch 9.29 sec
________________________________________________________________________________
```

## 10. å‚è€ƒ

- https://www.tensorflow.org/text/tutorials/text_generation
