# Text generation with an RNN

- [Text generation with an RNN](#text-generation-with-an-rnn)
  - [ç®€ä»‹](#ç®€ä»‹)
  - [åˆå§‹è®¾ç½®](#åˆå§‹è®¾ç½®)
    - [å¯¼å…¥åŒ…](#å¯¼å…¥åŒ…)
    - [ä¸‹è½½æ•°æ®é›†](#ä¸‹è½½æ•°æ®é›†)
    - [è¯»å–æ•°æ®](#è¯»å–æ•°æ®)
  - [å¤„ç†æ–‡æœ¬](#å¤„ç†æ–‡æœ¬)
    - [å‘é‡åŒ–æ–‡æœ¬](#å‘é‡åŒ–æ–‡æœ¬)
    - [é¢„æµ‹ä»»åŠ¡](#é¢„æµ‹ä»»åŠ¡)
    - [åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼](#åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼)
    - [åˆ›å»ºè®­ç»ƒ batches](#åˆ›å»ºè®­ç»ƒ-batches)
  - [æ„å»ºæ¨¡å‹](#æ„å»ºæ¨¡å‹)
  - [è¯•ç”¨æ¨¡å‹](#è¯•ç”¨æ¨¡å‹)
  - [è®­ç»ƒæ¨¡å‹](#è®­ç»ƒæ¨¡å‹)
    - [è®¾ç½® optimizer å’Œ loss function](#è®¾ç½®-optimizer-å’Œ-loss-function)
    - [è®¾ç½® checkpoints](#è®¾ç½®-checkpoints)
    - [å¼€å§‹è®­ç»ƒ](#å¼€å§‹è®­ç»ƒ)
  - [ç”Ÿæˆæ–‡æœ¬](#ç”Ÿæˆæ–‡æœ¬)
  - [è‡ªå®šä¹‰è®­ç»ƒ](#è‡ªå®šä¹‰è®­ç»ƒ)
  - [å‚è€ƒ](#å‚è€ƒ)

2022-02-11, 17:15
***

## ç®€ä»‹

ä¸‹é¢æ¼”ç¤ºä½¿ç”¨åŸºäºå­—ç¬¦çš„ RNN ç”Ÿæˆæ–‡æœ¬ã€‚ä½¿ç”¨ Andrej Karpathy çš„åšå®¢ [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) ä¸­ä½¿ç”¨çš„ Shakespeare çš„ä¸€ç¯‡æ–‡ç« ä½œä¸ºæ•°æ®é›†ã€‚ç»™å®šè¯¥æ•°æ®ä¸­çš„å­—ç¬¦åºåˆ—ï¼ˆ"Shakespear"ï¼‰ï¼Œè®­ç»ƒæ¨¡å‹é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼ˆ"e"ï¼‰ã€‚åå¤è°ƒç”¨æ¨¡å‹å¯ä»¥ç”Ÿæˆè¾ƒé•¿çš„æ–‡æœ¬åºåˆ—ã€‚

ä¸‹é¢ä½¿ç”¨ `tf.keras` å®ç°ï¼Œä»¥ä¸‹æ–‡æœ¬æ˜¯æ¨¡å‹è®­ç»ƒ 30 ä¸ª epoch åä½¿ç”¨æç¤º "Q" å¼€å§‹è·å¾—çš„è¾“å‡ºï¼š

```txt
QUEENE:
I had thought thou hadst a Roman; for the oracle,
Thus by All bids the man against the word,
Which are so weak of care, by old care done;
Your children were in your holy love,
And the precipitation through the bleeding throne.

BISHOP OF ELY:
Marry, and will, my lord, to weep in such a one were prettiest;
Yet now I was adopted heir
Of the world's lamentable day,
To watch the next way with his father with his face?

ESCALUS:
The cause why then we are all resolved more sons.

VOLUMNIA:
O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,
And love and pale as any will to that word.

QUEEN ELIZABETH:
But how long have I heard the soul for this world,
And show his hands of life be proved to stand.

PETRUCHIO:
I say he look'd on, if I must be content
To stay him from the fatal of our country's bliss.
His lordship pluck'd from this sentence then for prey,
And then let us twain, being the moon,
were she such a case as fills m
```

è™½ç„¶æœ‰äº›å¥å­è¯­æ³•æ­£ç¡®ï¼Œä½†æ˜¯å¤§å¤šæ•°å¥å­æ²¡æœ‰æ„ä¹‰ï¼Œè¯¥æ¨¡å‹æ²¡æœ‰å­¦ä¹ åˆ°å•è¯çš„å«ä¹‰ï¼Œä½†æ˜¯è€ƒè™‘åˆ°ï¼š

- æ¨¡å‹æ˜¯åŸºäºå­—ç¬¦çš„ã€‚æ¨¡å‹å¹¶ä¸çŸ¥é“å¦‚ä½•æ‹¼å†™è‹±æ–‡å•è¯ï¼Œç”šè‡³ä¸çŸ¥é“è¿™äº›å•è¯æ˜¯æ–‡æœ¬çš„åŸºæœ¬ç»„æˆï¼›
- è®­ç»ƒæ•°æ®é›†æ‰¹é‡è¾ƒå°ï¼ˆæ¯ä¸ª 100 å­—ç¬¦ï¼‰ã€‚

## åˆå§‹è®¾ç½®

### å¯¼å…¥åŒ…

```python
import tensorflow as tf

import numpy as np
import os
import time
```

### ä¸‹è½½æ•°æ®é›†

```python
path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
```

```sh
Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt
1122304/1115394 [==============================] - 0s 0us/step
1130496/1115394 [==============================] - 0s 0us/step
```

### è¯»å–æ•°æ®

é¦–å…ˆæŸ¥çœ‹æ–‡æœ¬ï¼š

```python
# Read, then decode for py2 compat.
text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
# length of text is the number of characters in it
print(f'Length of text: {len(text)} characters')
```

```sh
Length of text: 1115394 characters
```

æŸ¥çœ‹æ–‡æœ¬çš„å‰ 250 ä¸ªå­—ç¬¦ï¼š

```python
# Take a look at the first 250 characters in text
print(text[:250])
```

```sh
First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.
```

æ–‡ä»¶ä¸­å­—ç¬¦ç§ç±»æ•°ï¼š

```python
# The unique characters in the file
vocab = sorted(set(text))
print(f'{len(vocab)} unique characters')
```

```sh
65 unique characters
```

## å¤„ç†æ–‡æœ¬

### å‘é‡åŒ–æ–‡æœ¬

åœ¨è®­ç»ƒå‰éœ€è¦å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å€¼è¡¨ç¤ºã€‚

`tf.keras.layers.StringLookup` layer å¯ä»¥å°†å­—ç¬¦è½¬æ¢ä¸ºæ•°å­— IDï¼Œåªéœ€è¦å…ˆå°†æ–‡æœ¬æ‹†åˆ†ä¸º tokensï¼š

```python
example_texts = ['abcdefg', 'xyz']

chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')
chars
```

```sh
<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>
```

ç„¶ååˆ›å»º [tf.keras.layers.StringLookup](../api/tf/keras/layers/StringLookup.md) layerï¼š

```python
ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)
```

è¯¥ layer è´Ÿè´£å°† tokens è½¬æ¢ä¸ºæ•°å­— IDï¼š

```python
ids = ids_from_chars(chars)
ids
```

```sh
<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>
```

ç”±äºæ„å»ºæ¨¡å‹çš„ç›®çš„æ˜¯ç”Ÿæˆæ–‡æœ¬ï¼Œå› æ­¤è¿˜éœ€è¦é€†æ“ä½œï¼Œå³å°†æ•°å­—IDè½¬æ¢ä¸ºå­—ç¬¦ã€‚æ­¤æ—¶å¯ä»¥ä½¿ç”¨ [tf.keras.layers.StringLookup(..., invert=True)](../api/tf/keras/layers/StringLookup.md)ã€‚

ä¸ºäº†ä¿è¯ä¸¤ä¸ª `StringLookup` å…·æœ‰ç›¸åŒçš„è¯æ±‡è¡¨ï¼Œä¸‹é¢ä½¿ç”¨ `get_vocabulary()` è·å¾—ä¸Šé¢çš„è¯æ±‡è¡¨ï¼š

```python
chars_from_ids = tf.keras.layers.StringLookup(
    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)
```

è¯¥ layer å°† ID å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ï¼Œè¿”å›å­—ç¬¦ç±»å‹çš„ [tf.RaggedTensor](../api/tf/RaggedTensor.md)ï¼š

```python
chars = chars_from_ids(ids)
chars
```

```sh
<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>
```

å¯ä»¥ç”¨ [tf.strings.reduce_join](../api/tf/strings/reduce_join.md) å°†å­—ç¬¦è¿æ¥ä¸ºå­—ç¬¦ä¸²ï¼š

```python
tf.strings.reduce_join(chars, axis=-1).numpy()
```

```sh
array([b'abcdefg', b'xyz'], dtype=object)
```

```python
def text_from_ids(ids):
  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)
```

### é¢„æµ‹ä»»åŠ¡

ç»™å®šä¸€ä¸ªå­—ç¬¦æˆ–ä¸€ä¸²å­—ç¬¦ï¼Œä¸‹ä¸€ä¸ªæœ€å¯èƒ½çš„å­—ç¬¦æ˜¯ä»€ä¹ˆï¼Ÿè¿™å°±æ˜¯æ¨¡å‹æ‰€éœ€æ‰§è¡Œçš„ä»»åŠ¡ã€‚è¯¥æ¨¡å‹çš„è¾“å…¥æ˜¯ä¸€ä¸ªå­—ç¬¦åºåˆ—ï¼Œéœ€è¦è®­ç»ƒè¯¥æ¨¡å‹æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªæ—¶é—´æ­¥çš„å­—ç¬¦æ˜¯ä»€ä¹ˆã€‚

### åˆ›å»ºè®­ç»ƒæ ·æœ¬å’Œç›®æ ‡å€¼

ä¸‹é¢å°†æ–‡æœ¬åˆ’åˆ†ä¸ºæ ·æœ¬åºåˆ—ã€‚æ¯ä¸ªè¾“å…¥åºåˆ—ä¸ºæ¥è‡ªæ–‡æœ¬é•¿åº¦ä¸º `seq_length` å­—ç¬¦åºåˆ—ã€‚

å¯¹æ¯ä¸ªè¾“å…¥åºåˆ—ï¼Œå¯¹åº”çš„ç›®æ ‡åŒ…å«ç›¸åŒé•¿åº¦çš„æ–‡æœ¬ï¼Œåªæ˜¯å‘å³ç§»äº†ä¸€ä¸ªå­—ç¬¦ã€‚å‡è®¾ `seq_length` ä¸º 4ï¼Œæ–‡æœ¬ä¸º "Hello"ã€‚åˆ™è¾“å…¥ä¸º "Hell"ï¼Œç›®æ ‡åºåˆ—ä¸º "ello"ã€‚

ä¸ºæ­¤ï¼Œé¦–å…ˆä½¿ç”¨ [tf.data.Dataset.from_tensor_slices](../api/tf/data/Dataset.md#fromtensorslices) å‡½æ•°å°†æ–‡æœ¬å‘é‡è½¬æ¢ä¸ºå­—ç¬¦ç´¢å¼•æµã€‚

```python
all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))
all_ids
```

```sh
<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>
```

```python
ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)
```

```python
for ids in ids_dataset.take(10):
    print(chars_from_ids(ids).numpy().decode('utf-8'))
```

```sh
F
i
r
s
t
 
C
i
t
i
```

```python
seq_length = 100
examples_per_epoch = len(text)//(seq_length+1)
```

ä½¿ç”¨ `batch` æ–¹æ³•å¯ä»¥è½»æ¾å°†è¿™äº›å•ä¸ªå­—ç¬¦è½¬æ¢ä¸ºæŒ‡å®šé•¿åº¦çš„åºåˆ—ï¼š

```python
sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)

for seq in sequences.take(1):
  print(chars_from_ids(seq))
```

```sh
tf.Tensor(
[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'
 b'\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'
 b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'
 b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'
 b'e' b'a' b'k' b'.' b'\n' b'\n' b'A' b'l' b'l' b':' b'\n' b'S' b'p' b'e'
 b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\n' b'\n' b'F' b'i'
 b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\n' b'Y'
 b'o' b'u' b' '], shape=(101,), dtype=string)
```

å°†ä¸Šé¢çš„ tokens è¿æ¥æˆå­—ç¬¦ä¸²ï¼Œæ›´å®¹æ˜“çœ‹å‡ºæ•ˆæœï¼š

```python
for seq in sequences.take(5):
  print(text_from_ids(seq).numpy())
```

```sh
b'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
b'are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you k'
b"now Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us ki"
b"ll him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be d"
b'one: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citi'
```

ä¸ºäº†è®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦ `(input, label)` æˆå¯¹çš„æ•°æ®é›†ï¼Œ`input` å’Œ `label` éƒ½æ˜¯åºåˆ—ã€‚åœ¨æ¯ä¸ªæ—¶é—´æ­¥ï¼Œè¾“å…¥æ˜¯å½“å‰å­—ç¬¦ï¼Œè¾“å‡º label æ˜¯ä¸‹ä¸€ä¸ªå­—ç¬¦ã€‚

ä¸‹é¢çš„å‡½æ•°ï¼Œå°†è¾“å…¥åºåˆ—å¤åˆ¶å¹¶ç§»åŠ¨ 1 ä½ï¼Œä»è€Œå°†æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥å­—ç¬¦å’Œ label å­—ç¬¦å¯¹é½ï¼š

```python
def split_input_target(sequence):
    input_text = sequence[:-1]
    target_text = sequence[1:]
    return input_text, target_text
```

```python
split_input_target(list("Tensorflow"))
```

```sh
(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],
 ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])
```

```python
dataset = sequences.map(split_input_target)
```

```python
for input_example, target_example in dataset.take(1):
    print("Input :", text_from_ids(input_example).numpy())
    print("Target:", text_from_ids(target_example).numpy())
```

```sh
Input : b'First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou'
Target: b'irst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou '
```

### åˆ›å»ºè®­ç»ƒ batches

å‰é¢ä½¿ç”¨ [tf.data](../api/tf/data/tf.data.md) å°†æ–‡æœ¬æ‹†åˆ†ä¸ºåºåˆ—é›†åˆã€‚å°†è¾“å…¥è¾“å…¥æ¨¡å‹ä¹‹å‰ï¼Œè¿˜éœ€è¦å°†æ•°æ®æ‰“ä¹±ï¼Œå¹¶æ‰“åŒ…æˆ batchesã€‚

```python
# Batch size
BATCH_SIZE = 64

# Buffer size to shuffle the dataset
# (TF data is designed to work with possibly infinite sequences,
# so it doesn't attempt to shuffle the entire sequence in memory. Instead,
# it maintains a buffer in which it shuffles elements).
BUFFER_SIZE = 10000

dataset = (
    dataset
    .shuffle(BUFFER_SIZE)
    .batch(BATCH_SIZE, drop_remainder=True)
    .prefetch(tf.data.experimental.AUTOTUNE))

dataset
```

```sh
<PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>
```

## æ„å»ºæ¨¡å‹

ä¸‹é¢é€šè¿‡æ‰©å±• [keras.Model](../api/tf/keras/Model.md) ç±»å®šä¹‰æ¨¡å‹ã€‚

è¯¥æ¨¡å‹åŒ…å«ä¸‰å±‚ï¼š

- [tf.keras.layers.Embedding](../api/tf/keras/layers/Embedding.md)ï¼šè¾“å…¥å±‚ã€‚å¯è®­ç»ƒçš„æŸ¥æ‰¾è¡¨ï¼Œå°†å­—ç¬¦ ID æ˜ å°„åˆ°ç»´åº¦ä¸º `embedding_dim` çš„å‘é‡ï¼›
- [tf.keras.layers.GRU](../api/tf/keras/layers/GRU.md)ï¼šä¸€ç§ RNNï¼Œå¤§å°ä¸º `units=rnn_units`ï¼Œè¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ LSTMã€‚
- [tf.keras.layers.Dense](../api/tf/keras/layers/Dense.md)ï¼šè¾“å‡ºå±‚ï¼Œè¾“å‡º `vocab_size`ã€‚å®ƒä¸ºè¯æ±‡è¡¨çš„æ¯ä¸ªå­—ç¬¦è¾“å‡ºä¸€ä¸ª logitã€‚

```python
# ä»¥å­—ç¬¦è¡¨ç¤ºçš„è¯æ±‡è¡¨é•¿åº¦
vocab_size = len(vocab)

# åµŒå…¥ç»´åº¦
embedding_dim = 256

# RNN å•å…ƒæ•°
rnn_units = 1024
```

```python
class MyModel(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, rnn_units):
    super().__init__(self)
    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)
    self.gru = tf.keras.layers.GRU(rnn_units,
                                   return_sequences=True,
                                   return_state=True)
    self.dense = tf.keras.layers.Dense(vocab_size)

  def call(self, inputs, states=None, return_state=False, training=False):
    x = inputs
    x = self.embedding(x, training=training)
    if states is None:
      states = self.gru.get_initial_state(x)
    x, states = self.gru(x, initial_state=states, training=training)
    x = self.dense(x, training=training)

    if return_state:
      return x, states
    else:
      return x
```

```python
model = MyModel(
    # Be sure the vocabulary size matches the `StringLookup` layers.
    vocab_size=len(ids_from_chars.get_vocabulary()),
    embedding_dim=embedding_dim,
    rnn_units=rnn_units)
```

å¯¹æ¯ä¸ªå­—ç¬¦ï¼Œæ¨¡å‹æŸ¥æ‰¾åµŒå…¥ï¼Œå°†åµŒå…¥è¾“å…¥ GRU è¿è¡Œä¸€ä¸ªæ—¶é—´æ­¥ï¼Œå†è¾“å…¥ Dense å±‚ç”Ÿæˆ logits æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼š

![](images/2022-03-09-23-21-45.png)

> ğŸ˜Š:è¿™é‡Œä¹Ÿå¯ä»¥ä½¿ç”¨ [keras.Sequential](../api/tf/keras/Sequential.md) æ¨¡å‹ã€‚ä¸ºäº†ç¨åèƒ½ç”Ÿæˆæ–‡æœ¬ï¼Œéœ€è¦ç®¡ç† RNN å†…éƒ¨çŠ¶æ€ã€‚æå‰åŒ…å«çŠ¶æ€çš„è¾“å…¥å’Œè¾“å‡ºé€‰é¡¹æ¯”ç¨åé‡æ•´æ¨¡å‹è¦ç®€å•å¾—å¤šã€‚

## è¯•ç”¨æ¨¡å‹

è¿è¡Œæ¨¡å‹ï¼Œçœ‹çœ‹å®ƒçš„è¡Œä¸ºæ˜¯å¦ç¬¦åˆé¢„æœŸã€‚

é¦–å…ˆæ£€æŸ¥è¾“å‡º shapeï¼š

```python
for input_example_batch, target_example_batch in dataset.take(1):
    example_batch_predictions = model(input_example_batch)
    print(example_batch_predictions.shape, "# (batch_size, sequence_length, vocab_size)")
```

```sh
(64, 100, 66) # (batch_size, sequence_length, vocab_size)
```

åœ¨ä¸Šä¾‹ä¸­ï¼Œè¾“å‡ºåºåˆ—é•¿åº¦ä¸º 100ï¼Œä½†æ¨¡å‹å¯ä»¥å¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥ï¼š

```python
model.summary()
```

```sh
Model: "my_model"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       multiple                  16896     
                                                                 
 gru (GRU)                   multiple                  3938304   
                                                                 
 dense (Dense)               multiple                  67650     
                                                                 
=================================================================
Total params: 4,022,850
Trainable params: 4,022,850
Non-trainable params: 0
_________________________________________________________________
```

ä¸ºäº†ä»æ¨¡å‹è·å¾—å®é™…çš„é¢„æµ‹ï¼Œéœ€è¦ä»è¾“å‡ºåˆ†å¸ƒä¸­å–æ ·ï¼Œä»¥è·å¾—å®é™…å­—ç¬¦ç´¢å¼•ã€‚è¯¥åˆ†å¸ƒç”±è¯æ±‡è¡¨ä¸Šçš„ logit å®šä¹‰ã€‚

> ä»è¾“å‡ºåˆ†å¸ƒä¸­å–æ ·å¾ˆé‡è¦ï¼Œ

è¯•ä¸€ä¸‹è¿™æ‰¹æ•°æ®çš„ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼š

```python
sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)
sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()
```

åœ¨æ¯ä¸ªæ—¶é—´æ­¥é¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ç´¢å¼•ï¼š

```python
sampled_indices
```

```sh
array([29, 23, 11, 14, 42, 27, 56, 29, 14,  6,  9, 65, 22, 15, 34, 64, 44,
       41, 11, 51, 10, 44, 42, 56, 13, 50,  1, 33, 45, 23, 28, 43, 12, 62,
       45, 60, 43, 62, 38, 19, 50, 35, 19, 14, 60, 56, 10, 64, 39, 56,  2,
       51, 63, 42, 39, 64, 43, 20, 20, 17, 40, 15, 52, 46,  7, 25, 34, 43,
       11, 11, 31, 34, 38, 44, 22, 49, 23,  4, 27,  0, 31, 39,  5,  9, 43,
       58, 33, 30, 49,  6, 63,  5, 50,  4,  6, 14, 62,  3,  7, 35])
```

è§£ç ï¼Œçœ‹çœ‹è¿™ä¸ªæœªç»è®­ç»ƒçš„æ¨¡å‹é¢„æµ‹çš„æ–‡æœ¬ï¼š

```python
print("Input:\n", text_from_ids(input_example_batch[0]).numpy())
print()
print("Next Char Predictions:\n", text_from_ids(sampled_indices).numpy())
```

```sh
Input:
 b":\nWherein the king stands generally condemn'd.\n\nBAGOT:\nIf judgement lie in them, then so do we,\nBeca"

Next Char Predictions:
 b"PJ:AcNqPA'.zIBUyeb:l3ecq?k\nTfJOd;wfudwYFkVFAuq3yZq lxcZydGGDaBmg,LUd::RUYeIjJ\\(N[UNK]RZ&.dsTQj'x&k\\)'Aw!,V"
```

## è®­ç»ƒæ¨¡å‹

æ­¤æ—¶ï¼Œé—®é¢˜å¯ä»¥è§†ä¸ºä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»æ¨¡å‹ã€‚ç»™å®šå‰é¢çš„ RNN çŠ¶æ€å’Œå½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„ç±»åˆ«ã€‚

### è®¾ç½® optimizer å’Œ loss function

æ ‡å‡† [tf.keras.losses.sparse_categorical_crossentropy](../api/tf/keras/metrics/sparse_categorical_crossentropy.md) æŸå¤±å‡½æ•°é€‚åˆè¯¥æƒ…å†µï¼Œå®ƒåº”ç”¨äºé¢„æµ‹çš„æœ€åä¸€ä¸ªç»´åº¦ã€‚

ç”±äºæ¨¡å‹è¿”å› logitï¼Œæ‰€ä»¥è¦æ·»åŠ  `from_logits` æ ‡ç­¾ï¼š

```python
loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)
```

```python
example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)
print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
print("Mean loss:        ", example_batch_mean_loss)
```

```sh
Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)
Mean loss:         tf.Tensor(4.1895466, shape=(), dtype=float32)
```

åˆšåˆå§‹åŒ–çš„æ¨¡å‹è¾“å‡º logit æ¥è¿‘éšæœºåˆ†å¸ƒã€‚ä¸ºäº†è¯å®è¿™ä¸€ç‚¹ï¼Œå¯ä»¥æ£€æŸ¥å¹³å‡æŸå¤±çš„æŒ‡æ•°åº”è¯¥æ¥è¿‘è¯æ±‡è¡¨çš„å¤§å°ã€‚æ›´é«˜çš„æŸå¤±å‡å€¼æ„å‘³ç€æ¨¡å‹ç¡®å®šå…¶ç­”æ¡ˆæ˜¯é”™è¯¯çš„ï¼Œè¯´æ˜æ²¡åˆå§‹åŒ–å¥½ï¼š

```python
tf.exp(example_batch_mean_loss).numpy()
```

```sh
65.99286
```

ä½¿ç”¨ [tf.keras.Model.compile](../api/tf/keras/Model.md) é…ç½®è®­ç»ƒè¿‡ç¨‹ã€‚ä½¿ç”¨ [tf.keras.optimizers.Adam](../api/tf/keras/optimizers/Adam.md)å’Œä¸Šé¢çš„æŸå¤±å‡½æ•°ï¼š

```python
model.compile(optimizer='adam', loss=loss)
```

### è®¾ç½® checkpoints

ä½¿ç”¨ [tf.keras.callbacks.ModelCheckpoint](../api/tf/keras/callbacks/ModelCheckpoint.md) æ¥ä¿è¯è®­ç»ƒæœŸé—´ä¿å­˜æ£€æŸ¥ç‚¹ï¼š

```python
# Directory where the checkpoints will be saved
checkpoint_dir = './training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_prefix,
    save_weights_only=True)
```

### å¼€å§‹è®­ç»ƒ

## ç”Ÿæˆæ–‡æœ¬

ä½¿ç”¨è¯¥æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„æœ€ç®€å•æ–¹æ³•æ˜¯åœ¨è®­ç»ƒä¸­è¿è¡Œï¼Œå¹¶åœ¨æ‰§è¡Œæ—¶è·Ÿè¸ªæ¨¡å‹å†…éƒ¨çŠ¶æ€ã€‚

![](images/2022-03-10-23-32-56.png)

æ¯æ¬¡è°ƒç”¨æ¨¡å‹ï¼Œä¼ å…¥ä¸€ä¸ªæ–‡æœ¬å’Œå†…éƒ¨çŠ¶æ€ï¼Œæ¨¡å‹è¿”å›ä¸‹ä¸€ä¸ªå­—ç¬¦çš„é¢„æµ‹åŠæ–°çŠ¶æ€ï¼Œå°†é¢„æµ‹å’ŒçŠ¶æ€ä¼ å›æ¨¡å‹ç»§ç»­ç”Ÿæˆæ–‡æœ¬ã€‚

ä»¥ä¸‹æ˜¯å•æ­¥é¢„æµ‹ï¼š

```python
class OneStep(tf.keras.Model):
  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):
    super().__init__()
    self.temperature = temperature
    self.model = model
    self.chars_from_ids = chars_from_ids
    self.ids_from_chars = ids_from_chars

    # åˆ›å»º mask ä»¥é¿å…ç”Ÿæˆ "[UNK]"
    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]
    sparse_mask = tf.SparseTensor(
        # Put a -inf at each bad index.
        values=[-float('inf')]*len(skip_ids),
        indices=skip_ids,
        # Match the shape to the vocabulary
        dense_shape=[len(ids_from_chars.get_vocabulary())])
    self.prediction_mask = tf.sparse.to_dense(sparse_mask)

  @tf.function
  def generate_one_step(self, inputs, states=None):
    # Convert strings to token IDs.
    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')
    input_ids = self.ids_from_chars(input_chars).to_tensor()

    # Run the model.
    # predicted_logits.shape is [batch, char, next_char_logits]
    predicted_logits, states = self.model(inputs=input_ids, states=states,
                                          return_state=True)
    # Only use the last prediction.
    predicted_logits = predicted_logits[:, -1, :]
    predicted_logits = predicted_logits/self.temperature
    # Apply the prediction mask: prevent "[UNK]" from being generated.
    predicted_logits = predicted_logits + self.prediction_mask

    # Sample the output logits to generate token IDs.
    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)
    predicted_ids = tf.squeeze(predicted_ids, axis=-1)

    # Convert from token ids to characters
    predicted_chars = self.chars_from_ids(predicted_ids)

    # Return the characters and model state.
    return predicted_chars, states
```

## è‡ªå®šä¹‰è®­ç»ƒ



## å‚è€ƒ

- https://www.tensorflow.org/text/tutorials/text_generation
