# 模型评估与选择

2024-07-16
@author Jiawei Mao

***

## 泛化能力

错误率低、精度高、召回率高，视具体任务和使用者需求而异。

泛化能力：适用于 unseen instance 的能力。

算法或软件：先弄清楚你要什么

## 过拟合和欠拟合

前提条件：新数据和已有数据符合同一个规律。

**泛化误差**：在“未来”样本上的误差。

**经验误差**：在训练集上的误差，也称为“训练误差”。

- 泛化误差越小越好
- 经验误差是否越小越好？**NO**

过拟合（overfitting）和欠拟合（underfitting）：

- 过拟合：学了训练数据中太多东西
- 欠拟合：对训练数据中的内容学的不够

<img src="./images/image-20240716141812540.png" alt="image-20240716141812540" style="zoom:50%;" />

**过拟合是机器学习的核心内容**，对过拟合没有完全的解决方案，所有技术都是在缓解过拟合。

## 三大问题

三个关键问题：

- 如何获得测试结果——**评估方法**
- 如何评估性能优劣——**性能度量**
- 如何判断实质差别（统计意义上表现好）——**比较检验**

## 评估方法

**关键**：怎么获得测试集（test set）？

**测试集应该与训练集“互斥”**。

常见方法：

- 留出法（hold-out）
- 交叉验证法（cross validation, CV）
- 自助法（bootstrap）

### 留出法

<img src="./images/image-20240716143216020.png" alt="image-20240716143216020" style="zoom: 33%;" />

注意：

- 保持数据分布一致性（例如：分层采样），即保持训练集和测试中不同类别的样本比例一致
- 多次重复划分，例如，100 次随机划分，计算平均值
- 测试集不能太大，也不能太小（1/5-1/3）

test set 只是用于选择模型，当模型选择好后，再用完整数据训练模型，作为最终模型。

### k-折交叉验证法

hold out 方法的问题是，哪怕重复 100 次，依然可能有一部分数据从来没出现在训练集或测试集中。

k-折交叉验证法能解决该问题。

<img src="./images/image-20240716144639605.png" alt="image-20240716144639605" style="zoom: 50%;" />

由于切分本身可能会对结果有扰动，所以可以将 **10-折交叉验证**重复 10 次，称为 $10\times 10$ CV，也是做 100 次试验。

若 $k=m$，则得到“留一法”（leave-one-out, **LOO**），这样训练得到的模型与最后用所有数据得到的模型，会非常一致。根据 NFL 原理，这样也有问题，因为测试集只有 1 个样本，所以测试偏差可能会很大。

### 自助法

基于自助采样（**bootstrap sampling**），亦称“有放回抽样”、“可重复采样”。

原始数据有多少样本，就自助采样多少个样本作为训练集。有的样本多次出现，有的没采到。

训练集中没有出现的数据，可以作为 test set。

<img src="./images/image-20240716151117906.png" alt="image-20240716151117906" style="zoom: 50%;" />

按照这种方法，大概 36.8% 的数据在训练集中没有出现。

**优点**：训练集与原样本集同规模。

**缺点**：改变了数据分布。

当学习任务对数据分布的轻微变化比较鲁棒且数据较少时，这种方法挺合适。

boostrap 方法实用性很强。

## 调参与验证集

**算法的参数**：一般由人工设定，亦称“**超参数**”。

模型的参数：一般由 算法学习确定。

调参过程：先产生若干模型，然后基于某种评估方法进行选择。

模型选择：调参数和选不同的算法，都是在进行模型选择。模型选择，就是确定机器学习变量的过程。

**训练集 vs. 测试集 vs. 验证集（validation set）** 

- 测试集用来看模型性能

- 验证集专门用来**调参数**（训练集中分出来调参数的部分）

算法参数选定后，要用 “训练集+验证集” 重新训练得到最终模型。将该模型放到测试集上，得到模型性能。

## 性能度量

**性能度量**（performance measure）是衡量模型泛化能力的评价标准，反映了任务需求。

使用不同的性能度量往往会导致不同的评判结果。

> [!NOTE]
>
> 什么样的模型是好的，不仅取决于算法和数据，还取决于任务需求。

**回归**（regression）任务常用**均方误差**：
$$
E(f;D)=\frac{1}{D}\sum_{i=1}^m (f(x_i)-y_i)^2
$$
**分类**（classification）常用 **错误率**（分类错误的比例）：
$$
E(f;D)=\frac{1}{D}\sum_{i=1}^m I(f(x_i)\ne y_i)
$$
**精度（accuracy）**：
$$
acc(f;D)=1-E(f;D)
$$
精度和错误率刚好反过来，比如错误率为 5%，那么精度就是 95%。

**混淆矩阵**：

<img src="./images/image-20240716154002691.png" alt="image-20240716154002691" style="zoom:50%;" />

**查准率（precision）**：
$$
P=\frac{TP}{TP+FP}
$$
预测为 positive 的结果里，真正 positive 的比例。

**查全率（recall）**:
$$
R=\frac{TP}{TP+FN}
$$
所有 positive 样本中，有多少被模型找出来了。

**F1 度量**

将 P 和 R 合并在一起，又得到一个指标 **F1 度量**：
$$
\begin{aligned}
F_1&=\frac{2\times P\times R}{P+R}\\
&=\frac{2\times TP}{样例总数+TP-TN}
\end{aligned}
$$
换个写法更好记：
$$
\frac{1}{F_1}=\frac{1}{2}\times(\frac{1}{P}+\frac{1}{R})
$$

这种方法称为**调和平均**。该平均不会忽视较小的值。

若对查准率/查全率有不同偏好：
$$
F_{\beta}=\frac{(1+\beta^2)\times P\times R}{(\beta^2\times P)+R}
$$

$$
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^2}\cdot (\frac{1}{P}+\frac{\beta^2}{R})
$$

$\beta > 1$ 时查全率（R）有更大影响；$\beta < 1$ 时查准率（P）有更大影响。

## 比较检验

在某种度量下取得评估结果后，是否可以直接比较评判优劣？

**通常不行**，因为：

- 测试性能不等于泛化性能
- 测试性能随着测试集的变化而变化
- 很多机器学习算法本身有一定的随机性（比如神经网络的参数初始化）

机器学习——**概率近似正确**：以很大的概率，得到一个很可能很好的模型。

统计假设检验（hypothesis test）为模型性能比较提供了重要依据。

**两学习器比较**：

- 交叉验证 t 检验（基于成对 t 检验）
  - k 折交叉验证：5x2 交叉验证
- McNemar 检验（基于列联表，卡方检验）

比如两个模型 A 和 B，在 10 折较差检验中，得到的误差分别为：

|      | A                   | B                   | Delta      |
| ---- | ------------------- | ------------------- | ---------- |
| 1    | $\text{Error}_{A1}$ | $\text{Error}_{B1}$ | $\delta_1$ |
| 2    | $\text{Error}_{A2}$ | $\text{Error}_{B2}$ | $\delta_2$ |
| 3    | $\text{Error}_{A3}$ | $\text{Error}_{B3}$ | $\delta_3$ |
| ...  |                     |                     |            |

如果 A 和 B 没有区别，那么它们的误差应该没有区别，$\delta$ 的均值接近 0，可以直接用 t 检验实现。

在 McNemar 检验中，列出：

|      | A    |      |
| ---- | ---- | ---- |
| B    | TP   | FN   |
|      | FP   | TN   |

TP 表示 A 和 B 都预测为 Positive 的样本；TN 表示 A 和 B 都预测为 negative 的样本。

FP 和 FN 则是 A 和 B 预测结果不相同的样本。然后用卡方检验。

**多学习器比较**

